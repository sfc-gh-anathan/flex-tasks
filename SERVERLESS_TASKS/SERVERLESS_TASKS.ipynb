{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "2oovdz5yarkjzzumdytb",
   "authorId": "2872018607748",
   "authorName": "ANATHAN",
   "authorEmail": "adam.nathan@snowflake.com",
   "sessionId": "144a9b40-2d28-4f7d-b07f-6ead54a39b2f",
   "lastEditTime": 1748727112396
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "imports",
    "collapsed": true,
    "codeCollapsed": true
   },
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\nfrom croniter import croniter\nfrom datetime import datetime, timedelta\nimport pytz\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport nbformat\nimport numpy as np\nfrom snowflake.snowpark.context import get_active_session\n\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "2a819af6-77e2-4be8-be00-b5ffaec9fefd",
   "metadata": {
    "language": "sql",
    "name": "get_serverless_tasks",
    "collapsed": true,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "show tasks in account;\nset QUERY_ID_TASKS = last_query_id();\n\nselect * FROM TABLE(RESULT_SCAN($QUERY_ID_TASKS));",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e566b139-d842-4216-850b-89d31cd818bd",
   "metadata": {
    "language": "sql",
    "name": "core_information",
    "collapsed": true,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "WITH high_level_details AS (\n  SELECT\n    \"name\" AS TASK_NAME,\n    \"id\" AS TASK_ID,\n    \"database_name\" AS DATABASE_NAME,\n    \"database_name\" || '.' || \"schema_name\" || '.' || \"name\" AS FQ_TASK_NAME,\n    \"schema_name\" AS SCHEMA_NAME,\n    \"owner\" AS OWNER,\n    \"warehouse\" AS WAREHOUSE,\n    \"schedule\" AS SCHEDULE,\n    \"predecessors\" AS PREDECESSORS,\n    \"definition\" AS DEFINITION,\n    \"task_relations\" AS TASK_RELATIONS,\n    \"scheduling_mode\" AS SCHEDULING_MODE,\n    \"target_completion_interval\" AS TARGET_COMPLETION_INTERVAL,\n    CASE\n      WHEN NOT WAREHOUSE IS NULL THEN 'USER_MANAGED'\n      WHEN SCHEDULING_MODE ILIKE '%FLEXIBLE%' THEN 'FLEXIBLE'\n      ELSE 'SERVERLESS'\n    END AS TASK_TYPE\n  FROM\n    TABLE(RESULT_SCAN($QUERY_ID_TASKS))\n),\ntarget_interval AS (\n  SELECT\n    FQ_TASK_NAME,\n    CAST(\n      CASE\n        WHEN REGEXP_LIKE(\n          UPPER(TARGET_COMPLETION_INTERVAL),\n          '\\\\d+\\\\s*(MINUTE|M)'\n        ) THEN TO_NUMBER(\n          REGEXP_SUBSTR(TARGET_COMPLETION_INTERVAL, '\\\\d+')\n        )\n        WHEN REGEXP_LIKE(\n          UPPER(TARGET_COMPLETION_INTERVAL),\n          '\\\\d+\\\\s*(HOUR|H)'\n        ) THEN TO_NUMBER(\n          REGEXP_SUBSTR(TARGET_COMPLETION_INTERVAL, '\\\\d+')\n        ) * 60\n        WHEN REGEXP_LIKE(\n          UPPER(TARGET_COMPLETION_INTERVAL),\n          '\\\\d+\\\\s*(SECOND|S)'\n        ) THEN TO_NUMBER(\n          REGEXP_SUBSTR(TARGET_COMPLETION_INTERVAL, '\\\\d+')\n        ) / 60\n      END AS INT\n    ) AS TARGET_COMPLETION_INTERVAL_MINS\n  FROM\n    high_level_details\n)\nSELECT\n  hld.TASK_NAME,\n  hld.FQ_TASK_NAME,\n  hld.TASK_ID,\n  hld.OWNER,\n  hld.WAREHOUSE,\n  hld.DATABASE_NAME,\n  hld.SCHEMA_NAME,\n  hld.SCHEDULE,\n  CASE\n    WHEN hld.SCHEDULING_MODE ILIKE '%FLEXIBLE%' THEN hld.SCHEDULING_MODE\n    ELSE 'NONE'\n  END AS SCHEDULING_MODE,\n  hld.PREDECESSORS,\n  hld.DEFINITION,\n  t.TARGET_COMPLETION_INTERVAL_MINS,\n  hld.TASK_RELATIONS,\n  hld.TASK_TYPE\nFROM\n  target_interval AS t\n  JOIN high_level_details AS hld ON t.FQ_TASK_NAME = hld.FQ_TASK_NAME;\n\nSET\n  QUERY_ID_CORE_INFORMATION = LAST_QUERY_ID();\n  \nSELECT\n  *\nFROM\n  TABLE(RESULT_SCAN($QUERY_ID_CORE_INFORMATION));",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1abe2f7e-1d63-43ba-b50a-56e2b014b358",
   "metadata": {
    "language": "sql",
    "name": "task_hierarchy",
    "collapsed": true,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "WITH show_tasks AS (\n  SELECT\n    *\n  FROM\n    TABLE(RESULT_SCAN($QUERY_ID_CORE_INFORMATION))\n),\ntasks\n/* Flatten predecessors (including tasks without predecessors) */\nAS (\n  SELECT\n    st.task_name,\n    st.fq_task_name,\n    st.task_id,\n    st.owner,\n    st.warehouse,\n    st.database_name,\n    st.schema_name,\n    st.schedule,\n    st.scheduling_mode,\n    st.predecessors,\n    st.definition,\n    st.target_completion_interval_mins,\n    st.task_relations,\n    st.task_type,\n    CAST(f.value AS TEXT) AS predecessor_task\n  FROM\n    show_tasks AS st,\n    LATERAL FLATTEN(\n      input => TRY_PARSE_JSON(CAST(st.predecessors AS VARIANT))\n    ) AS f(SEQ, KEY, PATH, INDEX, VALUE, THIS)\n  UNION ALL\n  SELECT\n    st.task_name,\n    st.fq_task_name,\n    st.task_id,\n    st.owner,\n    st.warehouse,\n    st.database_name,\n    st.schema_name,\n    st.schedule,\n    st.scheduling_mode,\n    st.predecessors,\n    st.definition,\n    st.target_completion_interval_mins,\n    st.task_relations,\n    st.task_type,\n    NULL AS predecessor_task\n  FROM\n    show_tasks AS st\n  WHERE\n    ARRAY_SIZE(TRY_PARSE_JSON(CAST(st.predecessors AS VARIANT))) = 0\n),\nrecursive_roots\n/* Recursive CTE to find root for each task */\n/* Final output with all columns */\nAS (\n  SELECT\n    task_name,\n    fq_task_name,\n    task_id,\n    owner,\n    warehouse,\n    database_name,\n    schema_name,\n    schedule,\n    scheduling_mode,\n    predecessors,\n    definition,\n    target_completion_interval_mins,\n    task_relations,\n    task_type,\n    predecessor_task,\n    task_name AS root_task,\n    fq_task_name AS fq_root_task\n  FROM\n    tasks\n  WHERE\n    predecessor_task IS NULL\n  UNION ALL\n  SELECT\n    t.task_name,\n    t.fq_task_name,\n    t.task_id,\n    t.owner,\n    t.warehouse,\n    t.database_name,\n    t.schema_name,\n    t.schedule,\n    t.scheduling_mode,\n    t.predecessors,\n    t.definition,\n    t.target_completion_interval_mins,\n    t.task_relations,\n    t.task_type,\n    t.predecessor_task,\n    r.root_task,\n    r.fq_root_task\n  FROM\n    tasks AS t\n    JOIN recursive_roots AS r ON t.predecessor_task = r.fq_task_name\n)\nSELECT\n  task_name\n  /* Basic Task Identifiers */,\n  fq_task_name,\n  task_id,\n  task_type,\n  database_name\n  /* Location and Ownership */,\n  schema_name,\n  owner,\n  warehouse,\n  schedule\n  /* Scheduling and Execution Details */,\n  scheduling_mode,\n  target_completion_interval_mins,\n  definition,\n  predecessors\n  /* Task Dependencies and Relations */,\n  task_relations,\n  fq_root_task\nFROM\n  recursive_roots\nGROUP BY\n  ALL\nORDER BY\n  fq_task_name;\n\nSET QUERY_ID_HIERARCHY = last_query_id();\n\nSELECT * FROM TABLE(RESULT_SCAN($QUERY_ID_HIERARCHY));",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "364d591e-8c7f-4962-9409-4e3ab052dd05",
   "metadata": {
    "language": "python",
    "name": "chart_display_information",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "# Import required packages\nimport streamlit as st\nimport plotly.express as px\nfrom snowflake.snowpark.context import get_active_session\n\n# Get the data from the query\nsession = get_active_session()\ndf = session.sql(\"\"\"\nWITH show_tasks AS (\n  SELECT\n    *\n  FROM\n    TABLE(RESULT_SCAN($QUERY_ID_CORE_INFORMATION))\n),\ntasks AS (\n  SELECT\n    st.task_name,\n    st.fq_task_name,\n    st.task_id,\n    st.owner,\n    st.warehouse,\n    st.database_name,\n    st.schema_name,\n    st.schedule,\n    st.scheduling_mode,\n    st.predecessors,\n    st.definition,\n    st.target_completion_interval_mins,\n    st.task_relations,\n    st.task_type,\n    CAST(f.value AS TEXT) AS predecessor_task\n  FROM\n    show_tasks AS st,\n    LATERAL FLATTEN(\n      input => TRY_PARSE_JSON(CAST(st.predecessors AS VARIANT))\n    ) AS f(SEQ, KEY, PATH, INDEX, VALUE, THIS)\n  UNION ALL\n  SELECT\n    st.task_name,\n    st.fq_task_name,\n    st.task_id,\n    st.owner,\n    st.warehouse,\n    st.database_name,\n    st.schema_name,\n    st.schedule,\n    st.scheduling_mode,\n    st.predecessors,\n    st.definition,\n    st.target_completion_interval_mins,\n    st.task_relations,\n    st.task_type,\n    NULL AS predecessor_task\n  FROM\n    show_tasks AS st\n  WHERE\n    ARRAY_SIZE(TRY_PARSE_JSON(CAST(st.predecessors AS VARIANT))) = 0\n),\nrecursive_roots AS (\n  SELECT\n    task_name,\n    fq_task_name,\n    task_id,\n    owner,\n    warehouse,\n    database_name,\n    schema_name,\n    schedule,\n    scheduling_mode,\n    predecessors,\n    definition,\n    target_completion_interval_mins,\n    task_relations,\n    task_type,\n    predecessor_task,\n    task_name AS root_task,\n    fq_task_name AS fq_root_task\n  FROM\n    tasks\n  WHERE\n    predecessor_task IS NULL\n  UNION ALL\n  SELECT\n    t.task_name,\n    t.fq_task_name,\n    t.task_id,\n    t.owner,\n    t.warehouse,\n    t.database_name,\n    t.schema_name,\n    t.schedule,\n    t.scheduling_mode,\n    t.predecessors,\n    t.definition,\n    t.target_completion_interval_mins,\n    t.task_relations,\n    t.task_type,\n    t.predecessor_task,\n    r.root_task,\n    r.fq_root_task\n  FROM\n    tasks AS t\n    JOIN recursive_roots AS r ON t.predecessor_task = r.fq_task_name\n)\nSELECT\n  task_name,\n  fq_task_name,\n  task_id,\n  task_type,\n  database_name,\n  schema_name,\n  owner,\n  warehouse,\n  schedule,\n  scheduling_mode,\n  target_completion_interval_mins,\n  definition,\n  predecessors,\n  task_relations,\n  fq_root_task\nFROM\n  recursive_roots\nGROUP BY\n  ALL\nORDER BY\n  fq_task_name\n\"\"\").to_pandas()\n\n# Create database filter\ndatabases = sorted(df['DATABASE_NAME'].unique())\nselected_database = st.selectbox('Select Database', options=databases)\n\n# Filter tasks by selected database\nfiltered_df = df[df['DATABASE_NAME'] == selected_database]\n\n# Create task selector\ntasks = sorted(filtered_df['TASK_NAME'].unique())\nselected_task = st.selectbox('Select Task', options=tasks)\n\n# Get task details\ntask_details = filtered_df[filtered_df['TASK_NAME'] == selected_task].iloc[0]\n\n# Display task details in an organized layout\nst.markdown(\"### Task Details\", help=\"Detailed information about the selected task\")\n\n# Create a container for better spacing\nwith st.container():\n    # Basic Information and Location in one row\n    col1, col2, col3, col4, col5 = st.columns(5)\n    with col1:\n        st.metric(\"Type\", task_details['TASK_TYPE'], label_visibility=\"visible\", help=\"Task execution type\")\n    with col2:\n        st.metric(\"Owner\", task_details['OWNER'], label_visibility=\"visible\", help=\"Task owner\")\n    with col3:\n        st.metric(\"Warehouse\", task_details['WAREHOUSE'] if task_details['WAREHOUSE'] else 'N/A', label_visibility=\"visible\", help=\"Associated warehouse\")\n    with col4:\n        st.metric(\"Database\", task_details['DATABASE_NAME'], label_visibility=\"visible\", help=\"Database name\")\n    with col5:\n        st.metric(\"Schema\", task_details['SCHEMA_NAME'], label_visibility=\"visible\", help=\"Schema name\")\n\n    # Add custom CSS to reduce metric size\n    st.markdown(\"\"\"\n        <style>\n            [data-testid=\"stMetricValue\"] {\n                font-size: 1rem;\n            }\n            [data-testid=\"stMetricLabel\"] {\n                font-size: 0.8rem;\n            }\n        </style>\n    \"\"\", unsafe_allow_html=True)\n\n    # Scheduling Information\n    col1, col2, col3 = st.columns(3)\n    with col1:\n        st.metric(\"Schedule\", task_details['SCHEDULE'] if task_details['SCHEDULE'] else 'N/A', label_visibility=\"visible\", help=\"Task schedule\")\n    with col2:\n        st.metric(\"Mode\", task_details['SCHEDULING_MODE'], label_visibility=\"visible\", help=\"Scheduling mode\")\n    with col3:\n        if task_details['TARGET_COMPLETION_INTERVAL_MINS']:\n            st.metric(\"Target Interval (mins)\", f\"{task_details['TARGET_COMPLETION_INTERVAL_MINS']:.0f}\", label_visibility=\"visible\", help=\"Target completion interval\")\n\n    # Dependencies and Root Task\n    with st.expander(\"Dependencies and Full Task Name\"):\n        st.text(\"Full Task Name:\")\n        st.code(task_details['FQ_TASK_NAME'], language=None)\n        \n        st.text(\"Root Task:\")\n        st.code(task_details['FQ_ROOT_TASK'], language=None)\n        \n        st.text(\"Predecessors:\")\n        if task_details['PREDECESSORS']:\n            st.json(task_details['PREDECESSORS'])\n        else:\n            st.text(\"No predecessors\")\n\n    # Task Definition\n    with st.expander(\"Task Definition\"):\n        st.code(task_details['DEFINITION'])",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2fb9b7ab-d7ed-4f4f-a60a-683fb556b121",
   "metadata": {
    "language": "sql",
    "name": "task_times",
    "codeCollapsed": true,
    "collapsed": true
   },
   "outputs": [],
   "source": "/* Get task history for the last 30 days */\nSELECT\n  *\nFROM\n  snowflake.account_usage.task_history\nWHERE\n  query_start_time > DATEADD(DAY, -30, CURRENT_TIMESTAMP());\nSET\n  QUERY_ID_TASK_HISTORY = LAST_QUERY_ID();\n  /* Get execution times from task_history for all tasks */\nSELECT\n  DATE(SCHEDULED_TIME) AS DATE,\n  NAME AS TASK_NAME,\n  DATABASE_NAME || '.' || SCHEMA_NAME || '.' || NAME AS FQ_TASK_NAME,\n  SUM(DATEDIFF(SECOND, SCHEDULED_TIME, COMPLETED_TIME)) AS TOTAL_EXECUTION_SECONDS,\n  AVG(DATEDIFF(SECOND, SCHEDULED_TIME, COMPLETED_TIME)) AS AVG_EXECUTION_SECONDS,\n  COUNT(*) AS RUNS\nFROM\n  TABLE(RESULT_SCAN($QUERY_ID_TASK_HISTORY))\nGROUP BY\n  ALL\nORDER BY 2, 1 ;\nSET\n  QUERY_ID_RUN_TIMES = LAST_QUERY_ID();\nSELECT\n  *\nFROM\n  TABLE(RESULT_SCAN($QUERY_ID_RUN_TIMES));",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0488491e-30f7-42a8-befd-636cc91d6e25",
   "metadata": {
    "language": "python",
    "name": "chart_average_execution_seconds",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "# Import required packages\nimport streamlit as st\nimport plotly.express as px\nfrom snowflake.snowpark.context import get_active_session\n\n# Get the data from the previous SQL query\nsession = get_active_session()\ndf = session.sql(\"\"\"\nSELECT\n  DATE(SCHEDULED_TIME) AS DATE,\n  NAME AS TASK_NAME,\n  DATABASE_NAME || '.' || SCHEMA_NAME || '.' || NAME AS FQ_TASK_NAME,\n  AVG(DATEDIFF(SECOND, SCHEDULED_TIME, COMPLETED_TIME)) AS AVG_EXECUTION_SECONDS\nFROM\n  snowflake.account_usage.task_history\nWHERE\n  query_start_time > DATEADD(DAY, -30, CURRENT_TIMESTAMP())\n  and task_name not like 'CIS%'\nGROUP BY\n  DATE,\n  NAME,\n  DATABASE_NAME || '.' || SCHEMA_NAME || '.' || NAME\nORDER BY \n  DATE, TASK_NAME\n\"\"\").to_pandas()\n\n# Create the line chart\nfig = px.line(df, \n              x='DATE', \n              y='AVG_EXECUTION_SECONDS',\n              color='TASK_NAME',\n              title='Average Task Execution Time by Day',\n              labels={\n                  'DATE': 'Date',\n                  'AVG_EXECUTION_SECONDS': 'Average Execution Time (seconds)',\n                  'TASK_NAME': 'Task Name'\n              })\n\n# Customize the layout\nfig.update_layout(\n    height=600,\n    showlegend=True,\n    legend=dict(\n        yanchor=\"top\",\n        y=0.99,\n        xanchor=\"left\",\n        x=1.02\n    ),\n    hovermode='x unified'\n)\n\n# Display the chart\nst.plotly_chart(fig, use_container_width=True)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b3c4ef1d-6d87-4a2a-80ae-cfdbc87afb3e",
   "metadata": {
    "language": "python",
    "name": "chart_total_task_execution_time",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "# Import required packages\nimport streamlit as st\nimport plotly.express as px\nfrom snowflake.snowpark.context import get_active_session\n\n# Get the data from the previous SQL query\nsession = get_active_session()\ndf = session.sql(\"\"\"\nSELECT\n  DATE(SCHEDULED_TIME) AS DATE,\n  NAME AS TASK_NAME,\n  DATABASE_NAME || '.' || SCHEMA_NAME || '.' || NAME AS FQ_TASK_NAME,\n  SUM(DATEDIFF(SECOND, SCHEDULED_TIME, COMPLETED_TIME)) AS TOTAL_EXECUTION_SECONDS\nFROM\n  snowflake.account_usage.task_history\nWHERE\n  query_start_time > DATEADD(DAY, -30, CURRENT_TIMESTAMP())\n  and task_name not like 'CIS%'\nGROUP BY\n  DATE,\n  NAME,\n  DATABASE_NAME || '.' || SCHEMA_NAME || '.' || NAME\nORDER BY \n  DATE, TASK_NAME\n\"\"\").to_pandas()\n\n# Create the line chart\nfig = px.line(df, \n              x='DATE', \n              y='TOTAL_EXECUTION_SECONDS',\n              color='TASK_NAME',\n              title='Total Task Execution Time by Day',\n              labels={\n                  'DATE': 'Date',\n                  'TOTAL_EXECUTION_SECONDS': 'Total Execution Time (seconds)',\n                  'TASK_NAME': 'Task Name'\n              })\n\n# Customize the layout\nfig.update_layout(\n    height=600,\n    showlegend=True,\n    legend=dict(\n        yanchor=\"top\",\n        y=0.99,\n        xanchor=\"left\",\n        x=1.02\n    ),\n    hovermode='x unified'\n)\n\n# Display the chart\nst.plotly_chart(fig, use_container_width=True)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "af101f76-3a17-409b-b0f2-7330c075f05a",
   "metadata": {
    "language": "sql",
    "name": "cell2",
    "collapsed": true,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "-- WITH CORE_INFORMATION AS (\n--   SELECT\n--     *\n--   FROM\n--     TABLE(RESULT_SCAN($QUERY_ID_CORE_INFORMATION))\n-- ),\n-- HIERARCHY AS (\n--   SELECT\n--     *\n--   FROM\n--     TABLE(RESULT_SCAN($QUERY_ID_HIERARCHY))\n-- ),\n-- RUN_TIMES AS (\n--   SELECT\n--     *\n--   FROM\n--     TABLE(RESULT_SCAN($QUERY_ID_RUN_TIMES))\n-- )\n-- SELECT\n--   CI.TASK_NAME,\n--   CI.FQ_TASK_NAME,\n--   CI.OWNER,\n--   CI.WAREHOUSE,\n--   CI.DEFINITION,\n--   CI.SCHEDULE,\n--   CI.SCHEDULING_MODE,\n--   CI.TARGET_COMPLETION_INTERVAL_MINS,\n--   CI.TASK_TYPE,\n--   H.FQ_ROOT_TASK,\n--   RT.TOTAL_EXECUTION_SECONDS,\n--   RT.AVG_EXECUTION_SECONDS,\n--   RT.RUNS\n-- FROM\n--   CORE_INFORMATION AS CI\n--   JOIN HIERARCHY AS H ON CI.FQ_TASK_NAME = H.FQ_TASK_NAME\n--   LEFT JOIN RUN_TIMES AS RT ON RT.FQ_TASK_NAME = CI.FQ_TASK_NAME;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "49c35a75-827a-4799-9ce0-1f4702d1c0bf",
   "metadata": {
    "language": "sql",
    "name": "config_and_execution_times",
    "collapsed": true,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "WITH CORE_INFORMATION AS (\n  SELECT\n    *\n  FROM\n    TABLE(RESULT_SCAN($QUERY_ID_CORE_INFORMATION))\n),\nHIERARCHY AS (\n  SELECT\n    *\n  FROM\n    TABLE(RESULT_SCAN($QUERY_ID_HIERARCHY))\n),\nRUN_TIMES AS (\n  SELECT\n    *\n  FROM\n    TABLE(RESULT_SCAN($QUERY_ID_RUN_TIMES))\n)\nSELECT\n  CI.TASK_NAME\n  /* Basic Task Identifiers */,\n  CI.FQ_TASK_NAME,\n  CI.TASK_ID,\n  CI.TASK_TYPE,\n  CI.DATABASE_NAME\n  /* Location and Ownership */,\n  CI.SCHEMA_NAME,\n  CI.OWNER,\n  CI.WAREHOUSE,\n  CI.SCHEDULE\n  /* Task Configuration */,\n  CI.SCHEDULING_MODE,\n  CI.TARGET_COMPLETION_INTERVAL_MINS,\n  CI.DEFINITION,\n  CI.PREDECESSORS\n  /* Task Dependencies */,\n  CI.TASK_RELATIONS,\n  H.FQ_ROOT_TASK,\n  RT.TOTAL_EXECUTION_SECONDS\n  /* Execution Statistics */,\n  RT.AVG_EXECUTION_SECONDS,\n  RT.RUNS\nFROM\n  CORE_INFORMATION AS CI\n  JOIN HIERARCHY AS H ON CI.FQ_TASK_NAME = H.FQ_TASK_NAME\n  LEFT JOIN RUN_TIMES AS RT ON RT.FQ_TASK_NAME = CI.FQ_TASK_NAME;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "33fbe97c-c198-4504-b026-698633241dcd",
   "metadata": {
    "language": "sql",
    "name": "wh_query_credits",
    "collapsed": true,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "WITH TASK_HISTORY AS (\n  SELECT\n    DISTINCT QUERY_ID,\n    NAME AS TASK_NAME,\n    STATE,\n    DATABASE_NAME || '.' || SCHEMA_NAME || '.' || NAME AS FQ_TASK_NAME\n  FROM\n    TABLE(RESULT_SCAN($QUERY_ID_TASK_HISTORY))\n),\nQUERY_COST_DETAIL AS (\n  SELECT\n    TH.TASK_NAME,\n    FQ_TASK_NAME,\n    QH.QUERY_ID,\n    QH.START_TIME,\n    QH.DATABASE_NAME,\n    QH.SCHEMA_NAME,\n    QH.WAREHOUSE_NAME,\n    QH.WAREHOUSE_SIZE,\n    TOTAL_ELAPSED_TIME / 3600000.0 AS TOTAL_ELAPSED_HOURS,\n    CASE\n      UPPER(QH.WAREHOUSE_SIZE)\n      WHEN 'X-SMALL' THEN 1\n      WHEN 'SMALL' THEN 2\n      WHEN 'MEDIUM' THEN 4\n      WHEN 'LARGE' THEN 8\n      WHEN 'X-LARGE' THEN 16\n      WHEN '2X-LARGE' THEN 32\n      WHEN '3X-LARGE' THEN 64\n      WHEN '4X-LARGE' THEN 128\n      ELSE NULL\n    END AS CREDITS_PER_HOUR,\n    TOTAL_ELAPSED_HOURS * CREDITS_PER_HOUR AS ESTIMATED_CREDITS_USED\n  FROM\n    SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY AS QH\n    JOIN TASK_HISTORY AS TH ON TH.QUERY_ID = QH.QUERY_ID\n  WHERE\n    QH.START_TIME > DATEADD(DAY, -30, CURRENT_DATE)\n    AND NOT QH.WAREHOUSE_NAME IS NULL\n    AND NOT QH.DATABASE_NAME IS NULL\n    AND QH.DATABASE_NAME ILIKE 'SERVERLESS_TASKS'\n    AND NOT QH.WAREHOUSE_SIZE IS NULL\n    AND STATE = 'SUCCEEDED'\n)\nSELECT\n  TO_DATE(START_TIME) AS DATE,\n  TASK_NAME,\n  FQ_TASK_NAME,\n  WAREHOUSE_NAME,\n  UPPER(WAREHOUSE_SIZE) AS WAREHOUSE_SIZE,\n  COUNT(*) AS TASK_EXECUTION_COUNT,  \n  SUM(TOTAL_ELAPSED_HOURS) AS TOTAL_ELAPSED_HOURS,\n  SUM(ESTIMATED_CREDITS_USED) AS QUERY_CREDITS\nFROM\n  QUERY_COST_DETAIL\nGROUP BY\n  TO_DATE(START_TIME),\n  TASK_NAME,\n  FQ_TASK_NAME,\n  WAREHOUSE_NAME,\n  WAREHOUSE_SIZE\nORDER BY\n  TASK_NAME,\n  DATE;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cb192ca0-4a53-4503-806d-c3b8c986b6e8",
   "metadata": {
    "language": "python",
    "name": "chart_warehouse_credits",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "# Import required packages\nimport streamlit as st\nimport plotly.express as px\nfrom snowflake.snowpark.context import get_active_session\n\n# Get the data from the query\nsession = get_active_session()\ndf = session.sql(\"\"\"\nWITH TASK_HISTORY AS (\n  SELECT\n    DISTINCT QUERY_ID,\n    NAME AS TASK_NAME,\n    STATE,\n    DATABASE_NAME || '.' || SCHEMA_NAME || '.' || NAME AS FQ_TASK_NAME\n  FROM\n    TABLE(RESULT_SCAN($QUERY_ID_TASK_HISTORY))\n),\nQUERY_COST_DETAIL AS (\n  SELECT\n    TH.TASK_NAME,\n    FQ_TASK_NAME,\n    QH.QUERY_ID,\n    QH.START_TIME,\n    QH.DATABASE_NAME,\n    QH.SCHEMA_NAME,\n    QH.WAREHOUSE_NAME,\n    QH.WAREHOUSE_SIZE,\n    TOTAL_ELAPSED_TIME / 3600000.0 AS TOTAL_ELAPSED_HOURS,\n    CASE\n      UPPER(QH.WAREHOUSE_SIZE)\n      WHEN 'X-SMALL' THEN 1\n      WHEN 'SMALL' THEN 2\n      WHEN 'MEDIUM' THEN 4\n      WHEN 'LARGE' THEN 8\n      WHEN 'X-LARGE' THEN 16\n      WHEN '2X-LARGE' THEN 32\n      WHEN '3X-LARGE' THEN 64\n      WHEN '4X-LARGE' THEN 128\n      ELSE NULL\n    END AS CREDITS_PER_HOUR,\n    TOTAL_ELAPSED_HOURS * CREDITS_PER_HOUR AS ESTIMATED_CREDITS_USED\n  FROM\n    SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY AS QH\n    JOIN TASK_HISTORY AS TH ON TH.QUERY_ID = QH.QUERY_ID\n  WHERE\n    QH.START_TIME > DATEADD(DAY, -30, CURRENT_DATE)\n    AND NOT QH.WAREHOUSE_NAME IS NULL\n    AND NOT QH.DATABASE_NAME IS NULL\n    AND TH.TASK_NAME NOT LIKE '%CIS%'\n    AND NOT QH.WAREHOUSE_SIZE IS NULL\n    AND STATE = 'SUCCEEDED'\n)\nSELECT\n  TO_DATE(START_TIME) AS DATE,\n  TASK_NAME,\n  FQ_TASK_NAME,\n  WAREHOUSE_NAME,\n  UPPER(WAREHOUSE_SIZE) AS WAREHOUSE_SIZE,\n  COUNT(*) AS TASK_EXECUTION_COUNT,  \n  SUM(TOTAL_ELAPSED_HOURS) AS TOTAL_ELAPSED_HOURS,\n  SUM(ESTIMATED_CREDITS_USED) AS QUERY_CREDITS\nFROM\n  QUERY_COST_DETAIL\nGROUP BY\n  TO_DATE(START_TIME),\n  TASK_NAME,\n  FQ_TASK_NAME,\n  WAREHOUSE_NAME,\n  WAREHOUSE_SIZE\nORDER BY\n  TASK_NAME,\n  DATE\n\"\"\").to_pandas()\n\n# Create the line chart with custom hover data\nfig = px.line(df, \n              x='DATE', \n              y='QUERY_CREDITS',\n              color='TASK_NAME',\n              title='Query Credits Usage by Task and Day',\n              labels={\n                  'DATE': 'Date',\n                  'QUERY_CREDITS': 'Query Credits Used',\n                  'TASK_NAME': 'Task Name',\n                  'TOTAL_ELAPSED_HOURS': 'Total Hours'\n              },\n              hover_data={\n                  'TOTAL_ELAPSED_HOURS': ':.2f',\n                  'QUERY_CREDITS': ':.2f',\n                  'TASK_NAME': True,\n                  'DATE': True\n              })\n\n# Customize the layout\nfig.update_layout(\n    height=600,\n    showlegend=True,\n    legend=dict(\n        yanchor=\"top\",\n        y=0.99,\n        xanchor=\"left\",\n        x=1.02\n    ),\n    hovermode='x unified'\n)\n\n# Display the chart\nst.plotly_chart(fig, use_container_width=True)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bdad9a06-cd29-418c-92a5-93ffa1edb0c5",
   "metadata": {
    "language": "sql",
    "name": "serverless_query_credits",
    "collapsed": true,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "SELECT\n  TO_DATE (START_TIME) AS DATE,\n  TASK_NAME,\n  DATABASE_NAME || '.' || SCHEMA_NAME || '.' || TASK_NAME AS FQ_TASK_NAME,\n  COUNT(*) AS TASK_EXECUTION_COUNT,  \n  SUM(CREDITS_USED) AS TOTAL_SERVERLESS_CREDITS,\n  ROUND(TOTAL_SERVERLESS_CREDITS / TASK_EXECUTION_COUNT, 5) AS AVG_SERVERLESS_CREDITS_PER_RUN\nFROM\n  SNOWFLAKE.ACCOUNT_USAGE.SERVERLESS_TASK_HISTORY\nWHERE\n  START_TIME > DATEADD (DAY, -30, CURRENT_TIMESTAMP())\nGROUP BY\n  TO_DATE (START_TIME),\n  TASK_NAME,\n  DATABASE_NAME || '.' || SCHEMA_NAME || '.' || TASK_NAME\nORDER BY\n  TASK_NAME,\n  DATE;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "551b8b7a-e571-4ee6-ad71-393e884390d9",
   "metadata": {
    "language": "python",
    "name": "chart_serverless_credits",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "# Import required packages\nimport streamlit as st\nimport plotly.express as px\nfrom snowflake.snowpark.context import get_active_session\n\n# Get the data from the query\nsession = get_active_session()\ndf = session.sql(\"\"\"\nSELECT\n  TO_DATE(START_TIME) AS DATE,\n  TASK_NAME,\n  DATABASE_NAME || '.' || SCHEMA_NAME || '.' || TASK_NAME AS FQ_TASK_NAME,\n  COUNT(*) AS TASK_EXECUTION_COUNT,  \n  SUM(CREDITS_USED) AS TOTAL_SERVERLESS_CREDITS,\n  ROUND(TOTAL_SERVERLESS_CREDITS / TASK_EXECUTION_COUNT, 5) AS AVG_SERVERLESS_CREDITS_PER_RUN\nFROM\n  SNOWFLAKE.ACCOUNT_USAGE.SERVERLESS_TASK_HISTORY\nWHERE\n  START_TIME > DATEADD(DAY, -30, CURRENT_TIMESTAMP())\nGROUP BY\n  TO_DATE(START_TIME),\n  TASK_NAME,\n  DATABASE_NAME || '.' || SCHEMA_NAME || '.' || TASK_NAME\nORDER BY\n  TASK_NAME,\n  DATE\n\"\"\").to_pandas()\n\n# Create the line chart with custom hover data\nfig = px.line(df, \n              x='DATE', \n              y='TOTAL_SERVERLESS_CREDITS',\n              color='TASK_NAME',\n              title='Serverless Credits Usage by Task and Day',\n              labels={\n                  'DATE': 'Date',\n                  'TOTAL_SERVERLESS_CREDITS': 'Total Serverless Credits',\n                  'TASK_NAME': 'Task Name',\n                  'AVG_SERVERLESS_CREDITS_PER_RUN': 'Avg Credits per Run'\n              },\n              hover_data={\n                  'AVG_SERVERLESS_CREDITS_PER_RUN': ':.5f',\n                  'TOTAL_SERVERLESS_CREDITS': ':.2f',\n                  'TASK_NAME': True,\n                  'DATE': True,\n                  'TASK_EXECUTION_COUNT': True\n              })\n\n# Customize the layout\nfig.update_layout(\n    height=600,\n    showlegend=True,\n    legend=dict(\n        yanchor=\"top\",\n        y=0.99,\n        xanchor=\"left\",\n        x=1.02\n    ),\n    hovermode='x unified'\n)\n\n# Display the chart\nst.plotly_chart(fig, use_container_width=True)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1466d572-e237-4090-940b-1b167c06b8db",
   "metadata": {
    "language": "sql",
    "name": "auto_resume_credits",
    "collapsed": true,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "show warehouses;\nSET QUERY_ID_WAREHOUSES = last_query_id();\n\nWITH AUTO_SUSPEND_COST_LOOKUP AS (\n  SELECT\n    \"name\" AS WAREHOUSE_NAME,\n    \"auto_suspend\" AS AUTO_SUSPEND,\n    \"size\" AS SIZE,\n    CASE\n      UPPER(\"size\")\n      WHEN 'X-SMALL' THEN 1\n      WHEN 'SMALL' THEN 2\n      WHEN 'MEDIUM' THEN 4\n      WHEN 'LARGE' THEN 8\n      WHEN 'X-LARGE' THEN 16\n      WHEN '2X-LARGE' THEN 32\n      WHEN '3X-LARGE' THEN 64\n      WHEN '4X-LARGE' THEN 128\n      ELSE 0\n    END AS CREDIT_MULTIPLIER,\n    (AUTO_SUSPEND / 3600.0) * CREDIT_MULTIPLIER AS TOTAL_AUTO_RESUME_WH_CREDITS\n  FROM\n    TABLE(RESULT_SCAN($QUERY_ID_WAREHOUSES))\n),\nTASK_QUERIES AS (\n  SELECT\n    DISTINCT (QUERY_ID)\n  FROM\n    TABLE(INFORMATION_SCHEMA.TASK_HISTORY())\n  WHERE\n    ERROR_CODE IS NULL\n)\nSELECT\n  TO_DATE(TIMESTAMP) AS DATE,\n  WEH.WAREHOUSE_NAME,\n  UPPER(ASCL.SIZE) AS WAREHOUSE_SIZE,\n  ASCL.AUTO_SUSPEND,\n  ASCL.CREDIT_MULTIPLIER as wh_credit_multiplier,\n  COUNT(*) AS EVENT_COUNT,\n  SUM(TOTAL_AUTO_RESUME_WH_CREDITS) AS DAILY_AUTO_RESUME_CREDITS\nFROM\n  SNOWFLAKE.ACCOUNT_USAGE.WAREHOUSE_EVENTS_HISTORY AS WEH\n  JOIN AUTO_SUSPEND_COST_LOOKUP AS ASCL ON ASCL.WAREHOUSE_NAME = WEH.WAREHOUSE_NAME\n  JOIN TASK_QUERIES AS TQ ON TQ.QUERY_ID = WEH.QUERY_ID\nGROUP BY\n  DATE,\n  WEH.WAREHOUSE_NAME,\n  ASCL.SIZE,\n  ASCL.AUTO_SUSPEND,\n  ASCL.CREDIT_MULTIPLIER\nORDER BY\n  DATE,\n  WEH.WAREHOUSE_NAME;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1af933e6-598c-4188-9577-1d4536856f8f",
   "metadata": {
    "language": "python",
    "name": "chart_auto_suspend_cost",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "# Import required packages\nimport streamlit as st\nimport plotly.express as px\nfrom snowflake.snowpark.context import get_active_session\n\n# Get the data from the query\nsession = get_active_session()\ndf = session.sql(\"\"\"\nWITH AUTO_SUSPEND_COST_LOOKUP AS (\n  SELECT\n    \"name\" AS WAREHOUSE_NAME,\n    \"auto_suspend\" AS AUTO_SUSPEND,\n    \"size\" AS SIZE,\n    CASE\n      UPPER(\"size\")\n      WHEN 'X-SMALL' THEN 1\n      WHEN 'SMALL' THEN 2\n      WHEN 'MEDIUM' THEN 4\n      WHEN 'LARGE' THEN 8\n      WHEN 'X-LARGE' THEN 16\n      WHEN '2X-LARGE' THEN 32\n      WHEN '3X-LARGE' THEN 64\n      WHEN '4X-LARGE' THEN 128\n      ELSE 0\n    END AS CREDIT_MULTIPLIER,\n    (AUTO_SUSPEND / 3600.0) * CREDIT_MULTIPLIER AS TOTAL_AUTO_RESUME_WH_CREDITS\n  FROM\n    TABLE(RESULT_SCAN($QUERY_ID_WAREHOUSES))\n),\nTASK_QUERIES AS (\n  SELECT\n    DISTINCT (QUERY_ID)\n  FROM\n    TABLE(INFORMATION_SCHEMA.TASK_HISTORY())\n  WHERE\n    ERROR_CODE IS NULL\n)\nSELECT\n  TO_DATE(TIMESTAMP) AS DATE,\n  WEH.WAREHOUSE_NAME,\n  UPPER(ASCL.SIZE) AS WAREHOUSE_SIZE,\n  ASCL.AUTO_SUSPEND,\n  ASCL.CREDIT_MULTIPLIER as WH_CREDIT_MULTIPLIER,\n  COUNT(*) AS EVENT_COUNT,\n  SUM(TOTAL_AUTO_RESUME_WH_CREDITS) AS DAILY_AUTO_RESUME_CREDITS\nFROM\n  SNOWFLAKE.ACCOUNT_USAGE.WAREHOUSE_EVENTS_HISTORY AS WEH\n  JOIN AUTO_SUSPEND_COST_LOOKUP AS ASCL ON ASCL.WAREHOUSE_NAME = WEH.WAREHOUSE_NAME\n  JOIN TASK_QUERIES AS TQ ON TQ.QUERY_ID = WEH.QUERY_ID\nGROUP BY\n  DATE,\n  WEH.WAREHOUSE_NAME,\n  ASCL.SIZE,\n  ASCL.AUTO_SUSPEND,\n  ASCL.CREDIT_MULTIPLIER\nORDER BY\n  DATE,\n  WEH.WAREHOUSE_NAME\n\"\"\").to_pandas()\n\n# Create the line chart with custom hover data\nfig = px.line(df, \n              x='DATE', \n              y='DAILY_AUTO_RESUME_CREDITS',\n              color='WAREHOUSE_NAME',\n              title='Daily Auto-Resume Credits by Warehouse',\n              labels={\n                  'DATE': 'Date',\n                  'DAILY_AUTO_RESUME_CREDITS': 'Auto-Resume Credits',\n                  'WAREHOUSE_NAME': 'Warehouse Name',\n                  'WAREHOUSE_SIZE': 'Warehouse Size',\n                  'EVENT_COUNT': 'Resume Events'\n              },\n              hover_data={\n                  'WAREHOUSE_SIZE': True,\n                  'EVENT_COUNT': True,\n                  'AUTO_SUSPEND': True,\n                  'DAILY_AUTO_RESUME_CREDITS': ':.2f',\n                  'WAREHOUSE_NAME': True,\n                  'DATE': True\n              })\n\n# Customize the layout\nfig.update_layout(\n    height=600,\n    showlegend=True,\n    legend=dict(\n        yanchor=\"top\",\n        y=0.99,\n        xanchor=\"left\",\n        x=1.02\n    ),\n    hovermode='x unified'\n)\n\n# Display the chart\nst.plotly_chart(fig, use_container_width=True)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "30076b3a-b7da-433f-8a9f-fb779ce4876f",
   "metadata": {
    "language": "python",
    "name": "chart_total_cost_per_task",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "# Import required packages\nimport streamlit as st\nimport plotly.express as px\nfrom snowflake.snowpark.context import get_active_session\n\n# Get the data combining both serverless and warehouse credits\nsession = get_active_session()\ndf = session.sql(\"\"\"\nWITH SERVERLESS_COSTS AS (\n    SELECT\n        COALESCE(DATABASE_NAME, 'Unknown') AS DATABASE_NAME,\n        COALESCE(TASK_NAME, 'Unknown') AS TASK_NAME,\n        SUM(TOTAL_SERVERLESS_CREDITS) AS SERVERLESS_CREDITS\n    FROM (\n        SELECT\n            DATABASE_NAME,\n            TASK_NAME,\n            SUM(CREDITS_USED) AS TOTAL_SERVERLESS_CREDITS\n        FROM SNOWFLAKE.ACCOUNT_USAGE.SERVERLESS_TASK_HISTORY\n        WHERE START_TIME > DATEADD(DAY, -30, CURRENT_TIMESTAMP())\n        GROUP BY DATABASE_NAME, TASK_NAME\n    )\n    GROUP BY DATABASE_NAME, TASK_NAME\n),\nWAREHOUSE_COSTS AS (\n    SELECT \n        COALESCE(DATABASE_NAME, 'Unknown') AS DATABASE_NAME,\n        COALESCE(TASK_NAME, 'Unknown') AS TASK_NAME,\n        SUM(QUERY_CREDITS) AS WAREHOUSE_CREDITS\n    FROM (\n        WITH TASK_HISTORY AS (\n            SELECT DISTINCT \n                QUERY_ID,\n                DATABASE_NAME,\n                NAME AS TASK_NAME,\n                STATE\n            FROM TABLE(RESULT_SCAN($QUERY_ID_TASK_HISTORY))\n            WHERE STATE = 'SUCCEEDED'\n        )\n        SELECT\n            TH.DATABASE_NAME,\n            TH.TASK_NAME,\n            (QH.TOTAL_ELAPSED_TIME / 3600000.0) * \n            CASE UPPER(QH.WAREHOUSE_SIZE)\n                WHEN 'X-SMALL' THEN 1\n                WHEN 'SMALL' THEN 2\n                WHEN 'MEDIUM' THEN 4\n                WHEN 'LARGE' THEN 8\n                WHEN 'X-LARGE' THEN 16\n                WHEN '2X-LARGE' THEN 32\n                WHEN '3X-LARGE' THEN 64\n                WHEN '4X-LARGE' THEN 128\n                ELSE 0\n            END AS QUERY_CREDITS\n        FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY QH\n        JOIN TASK_HISTORY TH ON TH.QUERY_ID = QH.QUERY_ID\n        WHERE QH.START_TIME > DATEADD(DAY, -30, CURRENT_DATE)\n    )\n    GROUP BY DATABASE_NAME, TASK_NAME\n)\nSELECT \n    COALESCE(s.DATABASE_NAME, w.DATABASE_NAME) AS DATABASE_NAME,\n    COALESCE(s.TASK_NAME, w.TASK_NAME) AS TASK_NAME,\n    COALESCE(s.SERVERLESS_CREDITS, 0) AS SERVERLESS_CREDITS,\n    COALESCE(w.WAREHOUSE_CREDITS, 0) AS WAREHOUSE_CREDITS,\n    SERVERLESS_CREDITS + WAREHOUSE_CREDITS AS TOTAL_CREDITS\nFROM SERVERLESS_COSTS s\nFULL OUTER JOIN WAREHOUSE_COSTS w \n    ON s.DATABASE_NAME = w.DATABASE_NAME \n    AND s.TASK_NAME = w.TASK_NAME\nWHERE COALESCE(s.DATABASE_NAME, w.DATABASE_NAME) IS NOT NULL\n  AND COALESCE(s.TASK_NAME, w.TASK_NAME) IS NOT NULL\nORDER BY DATABASE_NAME, TASK_NAME\n\"\"\").to_pandas()\n\n# Handle empty dataframe case\nif df.empty:\n    st.warning(\"No task data available for the selected period.\")\nelse:\n    # Add database filter\n    databases = sorted(df['DATABASE_NAME'].unique())\n    selected_database = st.selectbox(\n        'Select Database',\n        options=databases,\n        key='database_selector'\n    )\n\n    # Filter tasks by selected database\n    db_filtered_df = df[df['DATABASE_NAME'] == selected_database]\n\n    # Add task selector\n    tasks = sorted(db_filtered_df['TASK_NAME'].unique())\n    selected_task = st.selectbox(\n        'Select Task',\n        options=tasks,\n        key='task_selector'\n    )\n\n    # Filter data based on task selection\n    filtered_df = db_filtered_df[db_filtered_df['TASK_NAME'] == selected_task]\n\n    # Create summary by task\n    task_summary = filtered_df.groupby('TASK_NAME').agg({\n        'SERVERLESS_CREDITS': 'sum',\n        'WAREHOUSE_CREDITS': 'sum',\n        'TOTAL_CREDITS': 'sum'\n    }).reset_index()\n\n    # Sort by total credits\n    task_summary = task_summary.sort_values('TOTAL_CREDITS', ascending=True)\n\n    # Create the horizontal bar chart\n    fig = px.bar(task_summary, \n                 y='TASK_NAME',\n                 x=['SERVERLESS_CREDITS', 'WAREHOUSE_CREDITS'],\n                 orientation='h',\n                 title=f'Credits Usage for Task: {selected_task}',\n                 labels={\n                     'value': 'Credits Used',\n                     'TASK_NAME': 'Task Name',\n                     'variable': 'Credit Type'\n                 },\n                 color_discrete_map={\n                     'SERVERLESS_CREDITS': '#1f77b4',\n                     'WAREHOUSE_CREDITS': '#ff7f0e'\n                 })\n\n    # Customize the layout\n    fig.update_layout(\n        height=max(400, len(task_summary) * 30),\n        barmode='stack',\n        showlegend=True,\n        legend_title_text='Credit Type',\n        xaxis_title='Total Credits',\n        yaxis={'categoryorder': 'total ascending'}\n    )\n\n    # Display the chart\n    st.plotly_chart(fig, use_container_width=True)\n\n    # Display summary metrics\n    st.markdown(\"### Summary Statistics\")\n    col1, col2, col3 = st.columns(3)\n    with col1:\n        st.metric(\"Total Credits\", f\"{task_summary['TOTAL_CREDITS'].sum():.2f}\")\n    with col2:\n        st.metric(\"Serverless Credits\", f\"{task_summary['SERVERLESS_CREDITS'].sum():.2f}\")\n    with col3:\n        st.metric(\"Warehouse Credits\", f\"{task_summary['WAREHOUSE_CREDITS'].sum():.2f}\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "90629f80-25ee-44b3-bb2d-9da583d71d92",
   "metadata": {
    "language": "python",
    "name": "chart_daily_credits_by_task",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "# Import required packages\nimport streamlit as st\nimport plotly.express as px\nfrom snowflake.snowpark.context import get_active_session\n\n# Get the data combining both serverless and warehouse credits\nsession = get_active_session()\ndf = session.sql(\"\"\"\nWITH SERVERLESS_COSTS AS (\n    SELECT\n        DATE,\n        TASK_NAME,\n        SUM(TOTAL_SERVERLESS_CREDITS) AS SERVERLESS_CREDITS\n    FROM (\n        SELECT\n            TO_DATE(START_TIME) AS DATE,\n            TASK_NAME,\n            SUM(CREDITS_USED) AS TOTAL_SERVERLESS_CREDITS\n        FROM SNOWFLAKE.ACCOUNT_USAGE.SERVERLESS_TASK_HISTORY\n        WHERE START_TIME > DATEADD(DAY, -30, CURRENT_TIMESTAMP())\n        GROUP BY DATE, TASK_NAME\n    )\n    GROUP BY DATE, TASK_NAME\n),\nWAREHOUSE_COSTS AS (\n    SELECT \n        DATE,\n        TASK_NAME,\n        SUM(QUERY_CREDITS) AS WAREHOUSE_CREDITS\n    FROM (\n        WITH TASK_HISTORY AS (\n            SELECT DISTINCT \n                QUERY_ID,\n                NAME AS TASK_NAME,\n                STATE\n            FROM TABLE(RESULT_SCAN($QUERY_ID_TASK_HISTORY))\n            WHERE STATE = 'SUCCEEDED'\n        )\n        SELECT\n            TO_DATE(QH.START_TIME) AS DATE,\n            TH.TASK_NAME,\n            (QH.TOTAL_ELAPSED_TIME / 3600000.0) * \n            CASE UPPER(QH.WAREHOUSE_SIZE)\n                WHEN 'X-SMALL' THEN 1\n                WHEN 'SMALL' THEN 2\n                WHEN 'MEDIUM' THEN 4\n                WHEN 'LARGE' THEN 8\n                WHEN 'X-LARGE' THEN 16\n                WHEN '2X-LARGE' THEN 32\n                WHEN '3X-LARGE' THEN 64\n                WHEN '4X-LARGE' THEN 128\n                ELSE 0\n            END AS QUERY_CREDITS\n        FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY QH\n        JOIN TASK_HISTORY TH ON TH.QUERY_ID = QH.QUERY_ID\n        WHERE QH.START_TIME > DATEADD(DAY, -30, CURRENT_DATE)\n    )\n    GROUP BY DATE, TASK_NAME\n)\nSELECT \n    COALESCE(s.DATE, w.DATE) AS DATE,\n    COALESCE(s.TASK_NAME, w.TASK_NAME) AS TASK_NAME,\n    COALESCE(s.SERVERLESS_CREDITS, 0) AS SERVERLESS_CREDITS,\n    COALESCE(w.WAREHOUSE_CREDITS, 0) AS WAREHOUSE_CREDITS,\n    SERVERLESS_CREDITS + WAREHOUSE_CREDITS AS TOTAL_CREDITS\nFROM SERVERLESS_COSTS s\nFULL OUTER JOIN WAREHOUSE_COSTS w \n    ON s.DATE = w.DATE \n    AND s.TASK_NAME = w.TASK_NAME\nORDER BY DATE, TASK_NAME\n\"\"\").to_pandas()\n\n# Add task selector\nall_tasks = sorted(df['TASK_NAME'].unique())\nselected_task = st.selectbox(\n    'Select Task to Display',\n    options=all_tasks,\n    key='single_task_selector'\n)\n\n# Filter data for selected task\ntask_df = df[df['TASK_NAME'] == selected_task]\n\n# Create the stacked bar chart\nfig = px.bar(task_df, \n             x='DATE',\n             y=['SERVERLESS_CREDITS', 'WAREHOUSE_CREDITS'],\n             title=f'Daily Credits Usage for Task: {selected_task}',\n             labels={\n                 'DATE': 'Date',\n                 'value': 'Credits Used',\n                 'variable': 'Credit Type'\n             },\n             color_discrete_map={\n                 'SERVERLESS_CREDITS': '#1f77b4',\n                 'WAREHOUSE_CREDITS': '#ff7f0e'\n             })\n\n# Customize the layout\nfig.update_layout(\n    height=500,\n    showlegend=True,\n    legend_title_text='Credit Type',\n    xaxis_title='Date',\n    yaxis_title='Credits',\n    barmode='stack'\n)\n\n# Display the chart\nst.plotly_chart(fig, use_container_width=True)\n\n# Display summary metrics for the selected task\nst.markdown(\"### Summary Statistics\")\ntask_summary = task_df.agg({\n    'SERVERLESS_CREDITS': 'sum',\n    'WAREHOUSE_CREDITS': 'sum',\n    'TOTAL_CREDITS': 'sum'\n}).round(2)\n\ncol1, col2, col3 = st.columns(3)\nwith col1:\n    st.metric(\"Total Credits\", f\"{task_summary['TOTAL_CREDITS']:.2f}\")\nwith col2:\n    st.metric(\"Serverless Credits\", f\"{task_summary['SERVERLESS_CREDITS']:.2f}\")\nwith col3:\n    st.metric(\"Warehouse Credits\", f\"{task_summary['WAREHOUSE_CREDITS']:.2f}\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "733d3586-d859-4d66-b6d5-47a0bdad128d",
   "metadata": {
    "language": "python",
    "name": "chart_largest_task_spend",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "# Import required packages\nimport streamlit as st\nimport plotly.express as px\nfrom snowflake.snowpark.context import get_active_session\nfrom datetime import datetime, timedelta\n\n# Add date range selector\ndate_range = st.selectbox(\n    'Select Time Period',\n    options=['Last 7 Days', 'Last 30 Days', 'Last 90 Days', 'All Time'],\n    key='date_range_selector'\n)\n\n# Get the data combining both serverless and warehouse credits\nsession = get_active_session()\ndf = session.sql(f\"\"\"\nWITH SERVERLESS_COSTS AS (\n    SELECT\n        TASK_NAME,\n        SUM(TOTAL_SERVERLESS_CREDITS) AS SERVERLESS_CREDITS\n    FROM (\n        SELECT\n            TASK_NAME,\n            SUM(CREDITS_USED) AS TOTAL_SERVERLESS_CREDITS\n        FROM SNOWFLAKE.ACCOUNT_USAGE.SERVERLESS_TASK_HISTORY\n        WHERE START_TIME > DATEADD(DAY, \n            CASE \n                WHEN '{date_range}' = 'Last 7 Days' THEN -7\n                WHEN '{date_range}' = 'Last 30 Days' THEN -30\n                WHEN '{date_range}' = 'Last 90 Days' THEN -90\n                ELSE -36500  -- ~100 years for \"All Time\"\n            END, \n            CURRENT_TIMESTAMP())\n        GROUP BY TASK_NAME\n    )\n    GROUP BY TASK_NAME\n),\nWAREHOUSE_COSTS AS (\n    SELECT \n        TASK_NAME,\n        SUM(QUERY_CREDITS) AS WAREHOUSE_CREDITS\n    FROM (\n        WITH TASK_HISTORY AS (\n            SELECT DISTINCT \n                QUERY_ID,\n                NAME AS TASK_NAME,\n                STATE\n            FROM TABLE(RESULT_SCAN($QUERY_ID_TASK_HISTORY))\n            WHERE STATE = 'SUCCEEDED'\n        )\n        SELECT\n            TH.TASK_NAME,\n            (QH.TOTAL_ELAPSED_TIME / 3600000.0) * \n            CASE UPPER(QH.WAREHOUSE_SIZE)\n                WHEN 'X-SMALL' THEN 1\n                WHEN 'SMALL' THEN 2\n                WHEN 'MEDIUM' THEN 4\n                WHEN 'LARGE' THEN 8\n                WHEN 'X-LARGE' THEN 16\n                WHEN '2X-LARGE' THEN 32\n                WHEN '3X-LARGE' THEN 64\n                WHEN '4X-LARGE' THEN 128\n                ELSE 0\n            END AS QUERY_CREDITS\n        FROM SNOWFLAKE.ACCOUNT_USAGE.QUERY_HISTORY QH\n        JOIN TASK_HISTORY TH ON TH.QUERY_ID = QH.QUERY_ID\n        WHERE QH.START_TIME > DATEADD(DAY, \n            CASE \n                WHEN '{date_range}' = 'Last 7 Days' THEN -7\n                WHEN '{date_range}' = 'Last 30 Days' THEN -30\n                WHEN '{date_range}' = 'Last 90 Days' THEN -90\n                ELSE -36500  -- ~100 years for \"All Time\"\n            END, \n            CURRENT_DATE)\n    )\n    GROUP BY TASK_NAME\n)\nSELECT \n    COALESCE(s.TASK_NAME, w.TASK_NAME) AS TASK_NAME,\n    COALESCE(s.SERVERLESS_CREDITS, 0) AS SERVERLESS_CREDITS,\n    COALESCE(w.WAREHOUSE_CREDITS, 0) AS WAREHOUSE_CREDITS,\n    SERVERLESS_CREDITS + WAREHOUSE_CREDITS AS TOTAL_CREDITS\nFROM SERVERLESS_COSTS s\nFULL OUTER JOIN WAREHOUSE_COSTS w ON s.TASK_NAME = w.TASK_NAME\nQUALIFY ROW_NUMBER() OVER (ORDER BY TOTAL_CREDITS DESC) <= 20\nORDER BY TOTAL_CREDITS DESC\n\"\"\").to_pandas()\n\n# Create the horizontal bar chart\nfig = px.bar(df, \n             y='TASK_NAME',\n             x=['SERVERLESS_CREDITS', 'WAREHOUSE_CREDITS'],\n             orientation='h',\n             title=f'Top 20 Tasks by Credit Usage ({date_range})',\n             labels={\n                 'value': 'Credits Used',\n                 'TASK_NAME': 'Task Name',\n                 'variable': 'Credit Type'\n             },\n             color_discrete_map={\n                 'SERVERLESS_CREDITS': '#1f77b4',\n                 'WAREHOUSE_CREDITS': '#ff7f0e'\n             })\n\n# Customize the layout\nfig.update_layout(\n    height=600,\n    barmode='stack',\n    showlegend=True,\n    legend_title_text='Credit Type',\n    xaxis_title='Total Credits',\n    yaxis={'categoryorder': 'total ascending'}\n)\n\n# Display the chart\nst.plotly_chart(fig, use_container_width=True)\n\n# Display summary metrics\nst.markdown(\"### Summary Statistics\")\ncol1, col2, col3 = st.columns(3)\nwith col1:\n    st.metric(\"Total Credits\", f\"{df['TOTAL_CREDITS'].sum():.2f}\")\nwith col2:\n    st.metric(\"Serverless Credits\", f\"{df['SERVERLESS_CREDITS'].sum():.2f}\")\nwith col3:\n    st.metric(\"Warehouse Credits\", f\"{df['WAREHOUSE_CREDITS'].sum():.2f}\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9aee60dd-86ed-4f4b-8477-833e6375e9ed",
   "metadata": {
    "language": "python",
    "name": "def_get_cron_times",
    "collapsed": true,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "def get_cron_times(schedule, sample_runs=100):\n    \"\"\"\n    Returns list of (weekday, minute of day) for a schedule expression.\n    Handles both CRON and interval-based schedules.\n\n    Args:\n        schedule (str): Schedule string (CRON or interval)\n        sample_runs (int): How many executions to sample\n\n    Returns:\n        List[Tuple[str, int]]: List of (weekday, minute_of_day)\n    \"\"\"\n    if schedule is None:\n        return []\n\n    # Handle interval-based schedules\n    interval_match = re.search(r'(\\d+)\\s*(MINUTE|MINUTES|HOUR|HOURS|SECOND|SECONDS)', schedule.upper())\n    if interval_match:\n        number, unit = interval_match.groups()\n        number = int(number)\n        \n        # Convert to minutes\n        if 'HOUR' in unit:\n            interval_minutes = number * 60\n        elif 'SECOND' in unit:\n            interval_minutes = number / 60\n        else:\n            interval_minutes = number\n\n        # Generate times for a week\n        times = []\n        start_time = datetime.now(pytz.utc)\n        current_time = start_time\n        \n        for _ in range(sample_runs):\n            weekday = current_time.strftime(\"%A\")\n            minute_of_day = current_time.hour * 60 + current_time.minute\n            times.append((weekday, minute_of_day))\n            current_time += timedelta(minutes=interval_minutes)\n            \n            # Stop if we've gone beyond a week\n            if (current_time - start_time).days >= 7:\n                break\n                \n        return times\n\n    # Handle CRON expressions\n    try:\n        # Extract timezone if present\n        tz_match = re.search(r'\\s+([A-Z]{3,4})$', schedule)\n        if tz_match:\n            timezone = tz_match.group(1)\n            # Remove timezone from schedule for croniter\n            schedule = schedule[:-(len(timezone)+1)].strip()\n            try:\n                tz = pytz.timezone(f'America/{timezone}')\n            except:\n                tz = pytz.timezone('UTC')\n        else:\n            tz = pytz.timezone('UTC')\n\n        # For CRON expressions with \"USING CRON\"\n        if 'USING CRON' in schedule.upper():\n            cron_part = re.search(r'USING CRON\\s+([^\"]+)', schedule)\n            if cron_part:\n                schedule = cron_part.group(1).strip()\n\n        iter = croniter(schedule, datetime.now(tz))\n        times = []\n\n        for _ in range(sample_runs):\n            next_run = iter.get_next(datetime)\n            # Convert to UTC for consistent display\n            if next_run.tzinfo is None:\n                next_run = tz.localize(next_run)\n            next_run_utc = next_run.astimezone(pytz.UTC)\n            weekday = next_run_utc.strftime(\"%A\")\n            minute_of_day = next_run_utc.hour * 60 + next_run_utc.minute\n            times.append((weekday, minute_of_day))\n\n        return times\n    except Exception as e:\n        st.warning(f\"Could not parse CRON expression: {schedule}. Error: {str(e)}\")\n        return []\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b062552d-b084-42b0-9c44-2b3e338151ca",
   "metadata": {
    "language": "python",
    "name": "job_run_heatmap",
    "collapsed": true,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "def plot_cron_heatmap(cron_times, cron_expr):\n    \"\"\"\n    Plot a static heatmap of cron job times using matplotlib and seaborn.\n    Color coding:\n    - Yellow: 2x duration < 60 minutes\n    - Red: 2x duration overlaps with next run\n    - Green: Otherwise\n    \"\"\"\n    if not cron_times:\n        print(\"No data to plot.\")\n        return\n\n    # Define the order of days for consistent Y-axis placement\n    day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n    day_to_num = {day: i for i, day in enumerate(day_order)}\n\n    # Create DataFrame from cron job times\n    df = pd.DataFrame(cron_times, columns=['Day', 'MinuteOfDay'])\n    df['DayNum'] = df['Day'].map(day_to_num)\n\n    # Create the heatmap data first\n    heatmap_data = df.groupby(['DayNum', 'MinuteOfDay']).size().unstack(fill_value=0)\n    \n    # Ensure all 1440 minutes are represented\n    all_minutes = list(range(0, 1440))\n    heatmap_data = heatmap_data.reindex(columns=all_minutes, fill_value=0)\n    \n    # Reorder rows so Sunday appears at the bottom\n    heatmap_data = heatmap_data.reindex(index=sorted(heatmap_data.index, reverse=True))\n\n    # Get unique start times to analyze intervals\n    start_times = []\n    for day, minute in set(cron_times):\n        day_num = day_to_num[day]\n        absolute_minute = day_num * 1440 + minute\n        start_times.append((day_num, minute, absolute_minute))\n    start_times.sort(key=lambda x: (x[0], x[1]))\n\n    # Calculate intervals and determine colors\n    duration_minutes = max(minute for _, minute in cron_times) - min(minute for _, minute in cron_times) + 1\n    double_duration = duration_minutes * 2\n\n    # Create color map based on conditions\n    color_map = {}\n    for i, (day_num, minute, abs_minute) in enumerate(start_times):\n        # Find next run time (wrap around to next day if needed)\n        next_idx = (i + 1) % len(start_times)\n        next_day_num, next_minute, next_abs_minute = start_times[next_idx]\n        \n        if next_idx == 0:  # Wrap to next week\n            next_abs_minute += 7 * 1440\n\n        interval = next_abs_minute - abs_minute if next_idx != 0 else float('inf')\n        \n        if double_duration < 60:\n            color = 'yellow'\n        elif interval < double_duration:\n            color = 'red'\n        else:\n            color = 'green'\n\n        for m in range(minute, minute + duration_minutes):\n            if m < 1440:\n                color_map[(day_num, m)] = color\n\n    # Create custom color matrix with softer colors\n    soft_red = (1, 0.7, 0.7)     # Light red\n    soft_yellow = (1, 1, 0.7)    # Light yellow\n    soft_green = (0.7, 1, 0.7)   # Light green\n    light_grey = (0.95, 0.95, 0.95)  # Light grey background\n\n    # Create custom color matrix with light grey background\n    color_matrix = np.full(heatmap_data.shape + (3,), light_grey)\n    for i, day_num in enumerate(heatmap_data.index):\n        for j, minute in enumerate(heatmap_data.columns):\n            if heatmap_data.iloc[i, j] > 0:\n                if (day_num, minute) in color_map:\n                    if color_map[(day_num, minute)] == 'red':\n                        color_matrix[i, j] = soft_red\n                    elif color_map[(day_num, minute)] == 'yellow':\n                        color_matrix[i, j] = soft_yellow\n                    else:\n                        color_matrix[i, j] = soft_green\n\n    # Create X-axis labels\n    xtick_labels = [f\"{m // 60:02d}:{m % 60:02d}\" for m in heatmap_data.columns]\n    ytick_labels = [day_order[i] for i in heatmap_data.index]\n\n    # Create the heatmap\n    plt.figure(figsize=(18, 6))\n    ax = plt.gca()\n    ax.imshow(color_matrix, aspect='auto')\n    \n    # Set labels and title\n    ax.set_xlabel('Time of Day')\n    ax.set_ylabel('Day of Week')\n    ax.set_title(f'Cron Format: {cron_expr}', fontsize=16, pad=20)\n    \n    # Set ticks\n    step = max(1, len(xtick_labels) // 24)\n    ax.set_xticks(range(0, len(xtick_labels), step))\n    ax.set_xticklabels(xtick_labels[::step], rotation=45, ha='right')\n    ax.set_yticks(range(len(ytick_labels)))\n    ax.set_yticklabels(ytick_labels)\n\n    # Set background colors\n    ax.set_facecolor(light_grey)\n    plt.gcf().patch.set_facecolor(light_grey)\n\n    plt.tight_layout()\n    plt.show()\n\n# Sample call\ncron_expr = \"0 9,11,13,15,17 * * 1-5\"  # Runs at 9 AM, 11 AM, 1 PM, 3 PM, and 5 PM on weekdays\nduration_minutes = 5  # Each job runs for 30 minutes\n\n# Get base schedule from cron expression\nbase_times = get_cron_times(cron_expr, sample_runs=25)  # 5 times per day * 5 weekdays\n\n# Create extended time slots to show duration\nextended_times = []\nfor day, start_minute in base_times:\n    # Add an entry for each minute of the job's duration\n    for minute in range(start_minute, start_minute + duration_minutes):\n        if minute < 1440:  # Ensure we don't exceed minutes in a day\n            extended_times.append((day, minute))\n\n# Plot the heatmap with the extended time slots\nplot_cron_heatmap(extended_times, cron_expr)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "83adf798-1975-4ef4-a40d-4d1f9eb9c792",
   "metadata": {
    "language": "python",
    "name": "get_min_interval",
    "collapsed": true,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "def get_minimum_interval_minutes(cron_times):\n    \"\"\"\n    Calculate the minimum time interval (in minutes) between any two job runs\n    in a weekly cron schedule.\n\n    Args:\n        cron_times (list of tuples): [(weekday_str, minute_of_day), ...]\n\n    Returns:\n        int or None: Minimum interval in minutes, or None if < 2 jobs exist.\n    \"\"\"\n    if len(cron_times) < 2:\n        return None\n\n    # Map days to numbers (0 = Monday ... 6 = Sunday)\n    day_to_num = {\n        'Monday': 0, 'Tuesday': 1, 'Wednesday': 2,\n        'Thursday': 3, 'Friday': 4, 'Saturday': 5, 'Sunday': 6\n    }\n\n    # Convert (day, minute_of_day) -> absolute minute in the week\n    absolute_minutes = [\n        day_to_num[day] * 1440 + minute for day, minute in cron_times\n    ]\n\n    # Sort and compute all pairwise differences (modulo 10080 to wrap week)\n    absolute_minutes.sort()\n    intervals = [\n        (absolute_minutes[i+1] - absolute_minutes[i]) for i in range(len(absolute_minutes) - 1)\n    ]\n\n    # Add wrap-around interval (last to first, looping into next week)\n    intervals.append((absolute_minutes[0] + 10080) - absolute_minutes[-1])\n\n    return min(intervals)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b7ec046b-2532-443a-9a86-cfb231548430",
   "metadata": {
    "language": "python",
    "name": "chart_task_schedule",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "# Import required packages\nimport streamlit as st\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom snowflake.snowpark.context import get_active_session\nfrom datetime import datetime, timedelta\nimport pytz\nimport re\nimport pandas as pd\nfrom croniter import croniter\n\n# Get task information with schedules and execution history\nsession = get_active_session()\ndf = session.sql(\"\"\"\nWITH task_executions AS (\n    SELECT \n        NAME AS TASK_NAME,\n        AVG(DATEDIFF(SECOND, SCHEDULED_TIME, COMPLETED_TIME)) AS AVG_EXECUTION_SECONDS,\n        COUNT(DISTINCT SCHEDULED_TIME) AS EXECUTION_COUNT,\n        MIN(SCHEDULED_TIME) AS FIRST_EXECUTION,\n        MAX(SCHEDULED_TIME) AS LAST_EXECUTION\n    FROM TABLE(RESULT_SCAN($QUERY_ID_TASK_HISTORY))\n    WHERE STATE = 'SUCCEEDED'\n    GROUP BY NAME\n)\nSELECT \n    t.TASK_NAME,\n    t.DATABASE_NAME,\n    t.SCHEMA_NAME,\n    t.SCHEDULE,\n    t.WAREHOUSE,\n    t.TASK_TYPE,\n    COALESCE(te.AVG_EXECUTION_SECONDS, 300) AS AVG_EXECUTION_SECONDS,\n    te.EXECUTION_COUNT,\n    te.FIRST_EXECUTION,\n    te.LAST_EXECUTION\nFROM TABLE(RESULT_SCAN($QUERY_ID_CORE_INFORMATION)) t\nLEFT JOIN task_executions te ON t.TASK_NAME = te.TASK_NAME\nWHERE t.SCHEDULE IS NOT NULL\nORDER BY t.DATABASE_NAME, t.TASK_NAME\n\"\"\").to_pandas()\n\n# Create form for filters\nwith st.form(\"schedule_filters\"):\n    col1, col2 = st.columns(2)\n    \n    with col1:\n        # Database filter\n        databases = sorted(df['DATABASE_NAME'].unique())\n        selected_database = st.selectbox('Select Database', options=databases)\n        \n        # Show only active tasks option\n        show_active = st.checkbox('Show Only Active Tasks', value=True)\n    \n    with col2:\n        # Schema filter\n        schemas = sorted(df[df['DATABASE_NAME'] == selected_database]['SCHEMA_NAME'].unique())\n        selected_schema = st.selectbox('Select Schema', options=schemas)\n        \n        # Minimum executions filter\n        min_executions = st.number_input('Minimum Executions', min_value=0, value=1)\n    \n    submitted = st.form_submit_button(\"Apply Filters\")\n\n# Filter data based on form inputs\ndf_filtered = df[\n    (df['DATABASE_NAME'] == selected_database) &\n    (df['SCHEMA_NAME'] == selected_schema)\n]\n\nif show_active:\n    df_filtered = df_filtered[df_filtered['EXECUTION_COUNT'] >= min_executions]\n\n# Add task selector\ntasks = sorted(df_filtered['TASK_NAME'].unique())\nif tasks:\n    selected_task = st.selectbox('Select Task to Display', options=tasks)\n    \n    # Filter for selected task\n    df_filtered = df_filtered[df_filtered['TASK_NAME'] == selected_task]\n\n# Process schedules and create visualization data\ntask_schedules = []\nfor _, row in df_filtered.iterrows():\n    try:\n        # Get next 7 days of runs\n        cron_times = get_cron_times(row['SCHEDULE'], sample_runs=35)  # 5 runs per day * 7 days\n        \n        for day, minute in cron_times:\n            # Convert minute of day to hours for better visualization\n            start_hour = minute / 60\n            runtime_hours = row['AVG_EXECUTION_SECONDS'] / 3600  # Convert seconds to hours\n            \n            task_schedules.append({\n                'Task': row['TASK_NAME'],\n                'Day': day,\n                'Start': start_hour,\n                'Duration': runtime_hours,\n                'Warehouse': row['WAREHOUSE'] or 'Serverless',\n                'Type': row['TASK_TYPE'],\n                'Executions': row['EXECUTION_COUNT'] or 0,\n                'Avg Runtime': f\"{runtime_hours:.2f} hours\",\n                'Schedule': row['SCHEDULE']\n            })\n    except Exception as e:\n        st.warning(f\"Could not process schedule for task {row['TASK_NAME']}: {e}\")\n\nif task_schedules:\n    # Create DataFrame for visualization\n    schedule_df = pd.DataFrame(task_schedules)\n    \n    # Summary metrics for selected task\n    st.markdown(f\"### Task Schedule: {selected_task}\")\n\n    # Custom CSS to reduce metric sizes\n    st.markdown(\"\"\"\n        <style>\n            div[data-testid=\"metric-container\"] {\n                padding: 10px !important;\n            }\n            \n            div[data-testid=\"metric-container\"] > div {\n                font-size: 12px !important;\n            }\n            \n            div[data-testid=\"metric-container\"] label {\n                font-size: 10px !important;\n                color: rgb(100, 100, 100) !important;\n            }\n            \n            div[data-testid=\"stHorizontalBlock\"] {\n                gap: 10px !important;\n            }\n        </style>\n    \"\"\", unsafe_allow_html=True)\n\n    # Create metrics in a row with smaller sizes\n    col1, col2, col3, col4 = st.columns(4)\n    with col1:\n        st.metric(\"Task Type\", schedule_df['Type'].iloc[0])\n    with col2:\n        st.metric(\"Warehouse\", schedule_df['Warehouse'].iloc[0])\n    with col3:\n        st.metric(\"Executions\", f\"{schedule_df['Executions'].iloc[0]:,.0f}\")\n    with col4:\n        st.metric(\"Avg Runtime\", schedule_df['Avg Runtime'].iloc[0])\n\n    # Display schedule expression\n    st.markdown(f\"**Schedule Expression:** `{schedule_df['Schedule'].iloc[0]}`\")\n\n    # Create Gantt-like chart using plotly\n    fig = go.Figure()\n\n    # Define day order for consistent display\n    day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n\n    # Color scheme based on task type\n    color_map = {\n        'SERVERLESS': '#1f77b4',\n        'USER_MANAGED': '#ff7f0e',\n        'FLEXIBLE': '#2ca02c'\n    }\n\n    # Add bars for the selected task\n    task_type = schedule_df['Type'].iloc[0]\n    fig.add_trace(go.Bar(\n        name=selected_task,\n        x=schedule_df['Duration'],\n        y=schedule_df['Day'],\n        orientation='h',\n        base=schedule_df['Start'],\n        marker_color=color_map.get(task_type, '#7f7f7f'),\n        customdata=schedule_df[['Task', 'Start', 'Duration', 'Warehouse', 'Type', 'Executions', 'Avg Runtime']],\n        hovertemplate=(\n            \"<b>%{customdata[0]}</b><br>\" +\n            \"Start Time: %{customdata[1]:.2f}<br>\" +\n            \"Duration: %{customdata[6]}<br>\" +\n            \"Warehouse: %{customdata[3]}<br>\" +\n            \"Type: %{customdata[4]}<br>\" +\n            \"Executions: %{customdata[5]}<br>\" +\n            \"Day: %{y}<br>\" +\n            \"<extra></extra>\"\n        )\n    ))\n\n    # Update layout\n    fig.update_layout(\n        title={\n            'text': f'Weekly Schedule for {selected_task}',\n            'y':0.95,\n            'x':0.5,\n            'xanchor': 'center',\n            'yanchor': 'top'\n        },\n        xaxis_title='Hour of Day',\n        yaxis_title='Day of Week',\n        height=400,\n        barmode='overlay',\n        yaxis={\n            'categoryorder': 'array', \n            'categoryarray': day_order,\n            'gridcolor': 'lightgrey'\n        },\n        xaxis={\n            'range': [0, 24],\n            'gridcolor': 'lightgrey',\n            'dtick': 1\n        },\n        plot_bgcolor='white',\n        showlegend=False,\n        hovermode='closest'\n    )\n\n    # Display the chart\n    st.plotly_chart(fig, use_container_width=True)\n\nelse:\n    st.warning(\"No schedule data available for the selected task.\")\n",
   "execution_count": null
  }
 ]
}