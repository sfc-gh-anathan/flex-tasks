{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "lyyax627vl7565ziun2b",
   "authorId": "2872018607748",
   "authorName": "ANATHAN",
   "authorEmail": "adam.nathan@snowflake.com",
   "sessionId": "af672907-5f19-48be-9e61-c820a50b671e",
   "lastEditTime": 1748702929737
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "imports"
   },
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\nfrom croniter import croniter\nfrom datetime import datetime, timedelta\nimport pytz\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport nbformat\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "2a819af6-77e2-4be8-be00-b5ffaec9fefd",
   "metadata": {
    "language": "sql",
    "name": "get_serverless_tasks"
   },
   "outputs": [],
   "source": "show tasks in account;\nset QUERY_ID_TASKS = last_query_id();\n\nselect * FROM TABLE(RESULT_SCAN($QUERY_ID_TASKS));",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e566b139-d842-4216-850b-89d31cd818bd",
   "metadata": {
    "language": "sql",
    "name": "core_information",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "-- get the high level details and normalize the target_completion_interval\n\nwith high_level_details as (\n   SELECT \n          \"name\" as task_name, \n          \"database_name\" || '.' || \"schema_name\" || '.' || \"name\" AS fq_task_name, \n          \"owner\" as owner, \"warehouse\" as warehouse, \n          \"schedule\" as schedule, \n          \"predecessors\" as predecessors, \"definition\" as definition, \n          \"task_relations\" as task_relations, \n          \"scheduling_mode\" as scheduling_mode,\n          \"target_completion_interval\" as target_completion_interval,\n     IFF(warehouse IS NULL, 'SERVERLESS', 'USER_MANAGED') AS task_type\n    FROM TABLE(RESULT_SCAN($QUERY_ID_TASKS))\n),\ntarget_interval as (\nSELECT\n    -- task_name,\n    fq_task_name,\n    CASE\n        WHEN UPPER(REGEXP_SUBSTR(target_completion_interval, '\\\\s*(\\\\d+)\\\\s*[A-Z]+')) IS NOT NULL THEN\n            CASE \n                WHEN REGEXP_LIKE(UPPER(target_completion_interval), '\\\\d+\\\\s*(MINUTE|M)') THEN \n                    TO_NUMBER(REGEXP_SUBSTR(target_completion_interval, '\\\\d+'))\n                WHEN REGEXP_LIKE(UPPER(target_completion_interval), '\\\\d+\\\\s*(HOUR|H)') THEN \n                    TO_NUMBER(REGEXP_SUBSTR(target_completion_interval, '\\\\d+')) * 60\n                WHEN REGEXP_LIKE(UPPER(target_completion_interval), '\\\\d+\\\\s*(SECOND|S)') THEN \n                    TO_NUMBER(REGEXP_SUBSTR(target_completion_interval, '\\\\d+')) / 60\n                ELSE NULL\n            END\n        ELSE NULL\n    END::integer AS target_completion_interval_mins\n FROM high_level_details\n )\n select \n -- *\n    hld.task_name, \n    hld.fq_task_name, \n    hld.owner, \n    hld.warehouse, \n    hld.schedule, \n    -- hld.scheduling_mode,\n    hld.definition, \n    t.target_completion_interval_mins, \n    hld.task_relations, \n  CASE\n    WHEN NOT WAREHOUSE IS NULL THEN 'USER_MANAGED'\n    WHEN SCHEDULING_MODE ILIKE '%FLEXIBLE%' THEN 'FLEXIBLE'\n    ELSE 'SERVERLESS'\n  END AS TASK_TYPE\n   from target_interval t\n   join high_level_details hld \n     on t.fq_task_name = hld.fq_task_name;\n\nSET QUERY_ID_CORE_INFORMATION = last_query_id();\n\nSELECT * \n  FROM TABLE(RESULT_SCAN($QUERY_ID_CORE_INFORMATION));",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1abe2f7e-1d63-43ba-b50a-56e2b014b358",
   "metadata": {
    "language": "sql",
    "name": "task_hierarchy",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "-- Get hierarchy\nWITH show_tasks AS (\n   SELECT \n     \"database_name\" AS database_name, \n     \"schema_name\" AS schema_name, \n     \"name\" AS task_name, \n     database_name || '.' || schema_name || '.' || \"name\" AS fq_task_name, \n     \"id\" AS task_id, \n     \"predecessors\" AS predecessors,\n     \"warehouse\" AS warehouse,\n     IFNULL(\"warehouse\", 'SERVERLESS') AS task_type\n   FROM TABLE(RESULT_SCAN($QUERY_ID_TASKS))\n),\n\n-- Flatten predecessors (including tasks without predecessors)\ntasks AS (\n    SELECT\n        st.task_name,\n        st.fq_task_name,\n        f.value::string AS predecessor_task,\n        st.warehouse,\n        st.task_type\n    FROM show_tasks st,\n         LATERAL FLATTEN(input => TRY_PARSE_JSON(st.predecessors::variant)) f\n\n    UNION ALL\n\n    SELECT\n        st.task_name,\n        st.fq_task_name,\n        NULL AS predecessor_task,\n        st.warehouse,\n        st.task_type        \n    FROM show_tasks st\n    WHERE ARRAY_SIZE(TRY_PARSE_JSON(st.predecessors::variant)) = 0\n),\n\n-- Recursive CTE to find root for each task\nrecursive_roots (\n    task_name, \n    fq_task_name, \n    predecessor_task, \n    root_task, \n    root_fq_task, \n    warehouse,\n    task_type\n) AS (\n\n    -- Base case: task is its own root\n    SELECT\n        task_name,\n        fq_task_name,\n        predecessor_task,\n        task_name AS root_task,\n        fq_task_name AS root_fq_task,\n        warehouse,\n        task_type\n    FROM tasks\n    WHERE predecessor_task IS NULL\n\n    UNION ALL\n\n    -- Recursive step: find root through predecessor\n    SELECT\n        t.task_name,\n        t.fq_task_name,\n        t.predecessor_task,\n        r.root_task,\n        r.root_fq_task,\n        t.warehouse,\n        t.task_type\n    FROM tasks t\n    JOIN recursive_roots r\n      ON t.predecessor_task = r.fq_task_name\n)\n\n-- Final output with root info\nSELECT \n    -- task_name,\n    fq_task_name,\n    -- predecessor_task,\n    -- root_task,\n    root_fq_task,\n    -- warehouse,\n    task_type\nFROM recursive_roots\nGROUP BY ALL\nORDER BY fq_task_name;\n\nSET QUERY_ID_HIERARCHY = last_query_id();\n\nselect * from table(result_scan($QUERY_ID_HIERARCHY));",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2fb9b7ab-d7ed-4f4f-a60a-683fb556b121",
   "metadata": {
    "language": "sql",
    "name": "task_times"
   },
   "outputs": [],
   "source": "select * from snowflake.account_usage.task_history where query_start_time > dateadd(d, -30, current_timestamp);\nSET QUERY_ID_TASK_HISTORY = last_query_id();\n\n-- get execution TIMES from task_history for all tasks\nSELECT name task_name, \n        database_name || '.' || schema_name || '.' || name fq_task_name, \n        sum(TIMESTAMPDIFF(s, scheduled_time, completed_time)) as total_execution_seconds,\n        avg(TIMESTAMPDIFF(s, scheduled_time, completed_time)) as avg_execution_seconds,   \n        count(*) runs\nFROM \nTABLE(RESULT_SCAN($QUERY_ID_TASK_HISTORY))\nwhere 1=1\n  -- AND database_name = 'SERVERLESS_TASKS'\n  -- and scheduled_time > dateadd(d, -30, current_timestamp())\ngroup by all;\n\nSET QUERY_ID_RUN_TIMES = last_query_id();",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "49c35a75-827a-4799-9ce0-1f4702d1c0bf",
   "metadata": {
    "language": "sql",
    "name": "config_and_execution_times"
   },
   "outputs": [],
   "source": "WITH CORE_INFORMATION AS (\n    SELECT * FROM TABLE(RESULT_SCAN($QUERY_ID_CORE_INFORMATION))\n),\nHIERARCHY AS (\n    SELECT * FROM TABLE(RESULT_SCAN($QUERY_ID_HIERARCHY))\n),\nRUN_TIMES AS (\n    SELECT * FROM TABLE(RESULT_SCAN($QUERY_ID_RUN_TIMES))\n)\nSELECT CI.TASK_NAME, \n       CI.FQ_TASK_NAME, \n       OWNER, \n       WAREHOUSE, \n       DEFINITION, \n       SCHEDULE, \n       SCHEDULING_MODE,\n       TARGET_COMPLETION_INTERVAL_MINS, \n       CI.TASK_TYPE, \n       TOTAL_EXECUTION_SECONDS, \n       AVG_EXECUTION_SECONDS, \n       RUNS\n  FROM CORE_INFORMATION ci\n  JOIN HIERARCHY h\n    ON ci.fq_task_name = h.fq_task_name\n  left JOIN RUN_TIMES rt\n    ON rt.fq_task_name = ci.fq_task_name",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "33fbe97c-c198-4504-b026-698633241dcd",
   "metadata": {
    "language": "sql",
    "name": "wh_query_credits"
   },
   "outputs": [],
   "source": "-- select * from snowflake.account_usage.task_history where query_start_time > dateadd(d, -30, current_timestamp);\n-- SET QUERY_ID_WH_QUERY_COST = last_query_id();\n-- SELECT $QUERY_ID_WH_QUERY_COST;\n\n\nSELECT \n    query_id,\n    database_name,\n    warehouse_name,\n    warehouse_size,\n    -- total_elapsed_time,  -- in milliseconds\n    total_elapsed_time / 3600000.0 AS total_elapsed_hours,\n    CASE UPPER(warehouse_size)\n        WHEN 'X-SMALL'   THEN 1\n        WHEN 'SMALL'     THEN 2\n        WHEN 'MEDIUM'    THEN 4\n        WHEN 'LARGE'     THEN 8\n        WHEN 'X-LARGE'   THEN 16\n        WHEN '2X-LARGE'  THEN 32\n        WHEN '3X-LARGE'  THEN 64\n        WHEN '4X-LARGE'  THEN 128\n        ELSE NULL\n    END AS credits_per_hour,\n    total_elapsed_hours * credits_per_hour estimated_credits_used\nFROM snowflake.account_usage.query_history\nWHERE start_time > dateadd(d, -30, current_date())\n  and warehouse_name IS NOT NULL \n  and database_name is not null\n  AND DATABASE_NAME = 'SERVERLESS_TASKS'\n  and warehouse_size is not null\n  AND query_id IN (\n      SELECT DISTINCT query_id \n      FROM TABLE(RESULT_SCAN($QUERY_ID_TASK_HISTORY))\n  );",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bdad9a06-cd29-418c-92a5-93ffa1edb0c5",
   "metadata": {
    "language": "sql",
    "name": "serverless_query_credits"
   },
   "outputs": [],
   "source": "SELECT\n  task_name,\n  database_name || '.' || schema_name || '.' || task_name AS fq_task_name,\n  SUM(credits_used) AS total_credits,\n  COUNT(*) AS task_execution_count,\n  ROUND(total_credits / task_execution_count, 5) AS avg_task_credits\nFROM\n  snowflake.account_usage.serverless_task_history\nWHERE\n  start_time > DATEADD (DAY, -30, CURRENT_TIMESTAMP())\n  -- AND database_name = 'SERVERLESS_TASKS'\nGROUP BY\n  1,\n  2\n order by 1 ;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1466d572-e237-4090-940b-1b167c06b8db",
   "metadata": {
    "language": "sql",
    "name": "auto_resume_credits"
   },
   "outputs": [],
   "source": "show warehouses;\nSET QUERY_ID_WAREHOUSES = last_query_id();\n\n-- get auto_resume costs. assume that if the query has triggered the warehouse, then the query needs to pay for the auto_suspend time.\n-- this is a lookup table where this auto resume is triggered.\nwith auto_suspend_cost_lookup as (\n    select \"name\" warehouse_name, \n    \"auto_suspend\" as auto_suspend, \n    \"size\" as size,   \n    CASE UPPER(\"size\")\n        WHEN 'X-SMALL' THEN 1\n        WHEN 'SMALL' THEN 2\n        WHEN 'MEDIUM' THEN 4\n        WHEN 'LARGE' THEN 8\n        WHEN 'X-LARGE' THEN 16\n        WHEN '2X-LARGE' THEN 32\n        WHEN '3X-LARGE' THEN 64\n        WHEN '4X-LARGE' THEN 128\n    ELSE 0\n  END AS credit_multiplier,\n  (auto_suspend / 3600.0) * credit_multiplier as total_auto_resume_wh_credits\n  FROM TABLE(RESULT_SCAN($QUERY_ID_WAREHOUSES))\n  ),\n-- warehouse events tied to tasks  \ntask_queries as (\n   SELECT distinct(query_id) FROM TABLE(INFORMATION_SCHEMA.TASK_HISTORY())where error_code is null\n)\n-- need to establish a task query that triggers a warehouse event (and note the query_id)\nselect timestamp, weh.query_id, weh.warehouse_name, auto_suspend, ascl.size, credit_multiplier, total_auto_resume_wh_credits\n  from SNOWFLAKE.ACCOUNT_USAGE.WAREHOUSE_EVENTS_HISTORY weh\n  join auto_suspend_cost_lookup ascl\n    on ascl.warehouse_name = weh.warehouse_name\n  join task_queries tq\n    on tq.query_id = weh.query_id\n -- where start_time > dateadd(d, -4, current_date());\n;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9aee60dd-86ed-4f4b-8477-833e6375e9ed",
   "metadata": {
    "language": "python",
    "name": "get_cron_times",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "def get_cron_times_if_safe(cron_expr, max_job_historical_run: timedelta, start_time=None, sample_runs=100):\n    \"\"\"\n    Returns list of (weekday, minute of day) for a cron expression,\n    but only if:\n    - It never runs more frequently than every 1 hour\n    - AND every interval is >= 2x max historical run time\n\n    Args:\n        cron_expr (str): Cron string like '*/15 * * * *'\n        max_job_historical_run (timedelta): Longest known runtime of the job\n        start_time (datetime, optional): Start of the interval. Defaults to now.\n        sample_runs (int): How many cron executions to sample\n\n    Returns:\n        List[Tuple[str, int]]: List of (weekday, minute_of_day) if valid\n                               Otherwise, an empty list []\n    \"\"\"\n    if start_time is None:\n        start_time = datetime.now(pytz.utc)\n\n    iter = croniter(cron_expr, start_time)\n    times = []\n\n    last_run = iter.get_next(datetime)\n    weekday = last_run.strftime(\"%A\")\n    minute_of_day = last_run.hour * 60 + last_run.minute\n    times.append((weekday, minute_of_day))\n\n    for _ in range(sample_runs - 1):\n        next_run = iter.get_next(datetime)\n        interval = next_run - last_run\n\n        # Check both minimum frequency and historical runtime safety margin\n        if interval < timedelta(hours=1) or interval < (2 * max_job_historical_run):\n            return []\n\n        weekday = next_run.strftime(\"%A\")\n        minute_of_day = next_run.hour * 60 + next_run.minute\n        times.append((weekday, minute_of_day))\n        last_run = next_run\n\n    return times",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a3a7d684-3e08-441c-971b-80100a4bb4ab",
   "metadata": {
    "language": "python",
    "name": "cron_test",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Cron: Daily at 9 AM\ncron_expr = \"0 9 * * *\"\nmax_runtime = timedelta(minutes=30)\n\nget_cron_times_if_safe(cron_expr, max_runtime)\n\n# print(get_cron_times_if_safe(cron_expr, max_runtime))  # ✅ Returns list\n\n# # Cron: Every hour\n# cron_expr2 = \"0 * * * *\"\n# max_runtime2 = timedelta(minutes=40)  # 2x = 80 mins > 60 mins interval\n\n# print(get_cron_times_if_safe(cron_expr2, max_runtime2))  # ❌ Returns []",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b062552d-b084-42b0-9c44-2b3e338151ca",
   "metadata": {
    "language": "python",
    "name": "job_run_heatmap",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef plot_cron_heatmap(cron_times):\n    \"\"\"\n    Plot a static heatmap of cron job times using matplotlib and seaborn.\n\n    Args:\n        cron_times (list of tuples): [(weekday_str, minute_of_day), ...]\n    \"\"\"\n    if not cron_times:\n        print(\"No data to plot.\")\n        return\n\n    # Define the order of days for consistent Y-axis placement\n    day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n    day_to_num = {day: i for i, day in enumerate(day_order)}\n\n    # Create DataFrame from cron job times\n    df = pd.DataFrame(cron_times, columns=['Day', 'MinuteOfDay'])\n    df['DayNum'] = df['Day'].map(day_to_num)\n\n    # Aggregate job counts per (day, minute) pair\n    heatmap_data = df.groupby(['DayNum', 'MinuteOfDay']).size().unstack(fill_value=0)\n\n    # Ensure all 1440 minutes are represented in columns\n    all_minutes = list(range(0, 1440))\n    heatmap_data = heatmap_data.reindex(columns=all_minutes, fill_value=0)\n\n    # Reorder rows so Sunday appears at the bottom\n    heatmap_data = heatmap_data.reindex(index=sorted(heatmap_data.index, reverse=True))\n\n    # Create X-axis labels (HH:MM format) for each minute\n    xtick_labels = [f\"{m // 60:02d}:{m % 60:02d}\" for m in heatmap_data.columns]\n    ytick_labels = [day_order[i] for i in heatmap_data.index]\n\n    # Create the heatmap\n    plt.figure(figsize=(18, 6))\n    ax = sns.heatmap(\n        heatmap_data,\n        cmap=\"YlGnBu\",\n        cbar_kws={'label': 'Job Runs'},\n        xticklabels=xtick_labels,\n        yticklabels=ytick_labels\n    )\n    ax.set_xlabel('Time of Day')\n    ax.set_ylabel('Day of Week')\n    ax.set_title('Cron Job Schedule Heatmap')\n\n    # Reduce number of X-axis labels to improve readability\n    step = max(1, len(xtick_labels) // 24)  # Approx. one label per hour\n    ax.set_xticks(range(0, len(xtick_labels), step))\n    ax.set_xticklabels(xtick_labels[::step], rotation=45, ha='right')\n\n    plt.tight_layout()\n    plt.show()\n\n\n\n\n# # Example usage with output from get_cron_times_if_safe\nexample_data = [\n    # Monday\n    (\"Monday\", 120),    # 02:00 AM\n    (\"Monday\", 540),    # 09:00 AM\n    (\"Monday\", 1020),   # 05:00 PM\n\n    # Tuesday\n    (\"Tuesday\", 75),    # 01:15 AM\n    (\"Tuesday\", 600),   # 10:00 AM\n    (\"Tuesday\", 1230),  # 08:30 PM\n\n    # Wednesday\n    (\"Wednesday\", 180),   # 03:00 AM\n    (\"Wednesday\", 720),   # 12:00 PM\n    (\"Wednesday\", 1320),  # 10:00 PM\n\n    # Thursday\n    (\"Thursday\", 240),   # 04:00 AM\n    (\"Thursday\", 480),   # 08:00 AM\n    (\"Thursday\", 1080),  # 06:00 PM\n\n    # Friday\n    (\"Friday\", 360),    # 06:00 AM\n    (\"Friday\", 660),    # 11:00 AM\n    (\"Friday\", 1380),   # 11:00 PM\n\n    # Saturday\n    (\"Saturday\", 90),    # 01:30 AM\n    (\"Saturday\", 780),   # 01:00 PM\n    (\"Saturday\", 1260),  # 09:00 PM\n\n    # Sunday\n    (\"Sunday\", 0),       # 12:00 AM\n    (\"Sunday\", 300),     # 05:00 AM\n    (\"Sunday\", 1140),    # 07:00 PM\n]\n\n\nplot_cron_heatmap(example_data)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "83adf798-1975-4ef4-a40d-4d1f9eb9c792",
   "metadata": {
    "language": "python",
    "name": "get_min_interval"
   },
   "outputs": [],
   "source": "def get_minimum_interval_minutes(cron_times):\n    \"\"\"\n    Calculate the minimum time interval (in minutes) between any two job runs\n    in a weekly cron schedule.\n\n    Args:\n        cron_times (list of tuples): [(weekday_str, minute_of_day), ...]\n\n    Returns:\n        int or None: Minimum interval in minutes, or None if < 2 jobs exist.\n    \"\"\"\n    if len(cron_times) < 2:\n        return None\n\n    # Map days to numbers (0 = Monday ... 6 = Sunday)\n    day_to_num = {\n        'Monday': 0, 'Tuesday': 1, 'Wednesday': 2,\n        'Thursday': 3, 'Friday': 4, 'Saturday': 5, 'Sunday': 6\n    }\n\n    # Convert (day, minute_of_day) -> absolute minute in the week\n    absolute_minutes = [\n        day_to_num[day] * 1440 + minute for day, minute in cron_times\n    ]\n\n    # Sort and compute all pairwise differences (modulo 10080 to wrap week)\n    absolute_minutes.sort()\n    intervals = [\n        (absolute_minutes[i+1] - absolute_minutes[i]) for i in range(len(absolute_minutes) - 1)\n    ]\n\n    # Add wrap-around interval (last to first, looping into next week)\n    intervals.append((absolute_minutes[0] + 10080) - absolute_minutes[-1])\n\n    return min(intervals)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9bf7ec46-b0c1-4f7d-a4ab-f0f4709a6096",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "outputs": [],
   "source": "def report_minimum_interval(cron_times, threshold=151):\n    min_interval = get_minimum_interval_minutes(cron_times)\n    print(min_interval)\n    if min_interval is None:\n        print(\"Not enough job runs to calculate interval.\")\n    else:\n        print(f\"Minimum interval between runs: {min_interval} minutes\")\n        if min_interval < threshold:\n            print(\"Task ineligible. Time frame too short\")\n            \nreport_minimum_interval(example_data)",
   "execution_count": null
  }
 ]
}